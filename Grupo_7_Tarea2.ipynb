{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Carpeta root del dataset\n",
    "image_path = 'BD_perros'\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                #transforms.Resize((28,28)),\n",
    "                                ])\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(image_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataset = Subset(dataset, torch.arange(309))\n",
    "test_dataset  = Subset(dataset, torch.arange(309, len(dataset)))\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size= batch_size, shuffle= True)\n",
    "valid_dl = DataLoader(test_dataset , batch_size= batch_size, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "# Conv -> ReLU -> MaxPooling\n",
    "model.add_module('conv1', nn.Conv2d(in_channels=3,out_channels=32,kernel_size=5,padding=2))\n",
    "model.add_module('relu1', nn.ReLU())\n",
    "model.add_module('pool1', nn.MaxPool2d(kernel_size=2))\n",
    "# Conv -> ReLU -> MaxPooling\n",
    "model.add_module('conv2', nn.Conv2d(in_channels=32,out_channels=64,kernel_size=5,padding=2))\n",
    "model.add_module('relu2', nn.ReLU())\n",
    "model.add_module('pool2', nn.MaxPool2d(kernel_size=2))\n",
    "# Flatten\n",
    "model.add_module('flatten', nn.Flatten())\n",
    "# Full Connected -> ReLU -> Dropout\n",
    "model.add_module('fc1', nn.Linear(3136,1024))\n",
    "model.add_module('relu3', nn.ReLU())\n",
    "model.add_module('dropout', nn.Dropout(p= .5))\n",
    "# Full Connected\n",
    "model.add_module('fc2', nn.Linear(1024, 10))\n",
    "#model.add_module('soft', nn.Softmax(dim=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're using: cuda as device.\n"
     ]
    }
   ],
   "source": [
    "# Selección de la unidad de procesamiento\n",
    "processing_unit = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(processing_unit)\n",
    "model.to(device)\n",
    "print(f\"You're using: {device} as device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, train_dl, valid_dl, device):\n",
    "    loss_hist_train = torch.zeros(num_epochs).to(device)\n",
    "    accuracy_hist_train = torch.zeros(num_epochs).to(device)\n",
    " \n",
    "    loss_hist_valid = torch.zeros(num_epochs).to(device)\n",
    "    accuracy_hist_valid = torch.zeros(num_epochs).to(device)\n",
    " \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_dl:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            pred = model(x_batch)\n",
    "            loss = loss_fn(pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss_hist_train[epoch] += loss.item() * y_batch.size(0)\n",
    "            is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "            accuracy_hist_train[epoch] += is_correct.sum()\n",
    "\n",
    "        loss_hist_train[epoch] /= len(train_dl.dataset)\n",
    "        accuracy_hist_train[epoch] /= len(train_dl.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in valid_dl:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                pred = model(x_batch)\n",
    "                loss = loss_fn(pred, y_batch)\n",
    "                loss_hist_valid[epoch] += loss.item() * y_batch.size(0)\n",
    "                is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "                accuracy_hist_valid[epoch] += is_correct.sum()\n",
    "            loss_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "            accuracy_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "        print(f'Epoch {epoch+1} accuracy: {accuracy_hist_train[epoch]:.4f} 'f'val_accuracy: {accuracy_hist_valid[epoch]:.4f}')\n",
    "    return loss_hist_train.cpu(), loss_hist_valid.cpu(), accuracy_hist_train.cpu(), accuracy_hist_valid.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sfons\\OneDrive - Universidad de La Frontera\\Universidad\\Sem8\\Análisis de Datos Práctico usando Python\\Tarea2\\Grupo_7_Tarea2.ipynb Celda 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sfons/OneDrive%20-%20Universidad%20de%20La%20Frontera/Universidad/Sem8/An%C3%A1lisis%20de%20Datos%20Pr%C3%A1ctico%20usando%20Python/Tarea2/Grupo_7_Tarea2.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sfons/OneDrive%20-%20Universidad%20de%20La%20Frontera/Universidad/Sem8/An%C3%A1lisis%20de%20Datos%20Pr%C3%A1ctico%20usando%20Python/Tarea2/Grupo_7_Tarea2.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sfons/OneDrive%20-%20Universidad%20de%20La%20Frontera/Universidad/Sem8/An%C3%A1lisis%20de%20Datos%20Pr%C3%A1ctico%20usando%20Python/Tarea2/Grupo_7_Tarea2.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m hist \u001b[39m=\u001b[39m train(model, num_epochs, train_dl, valid_dl, device)\n",
      "\u001b[1;32mc:\\Users\\sfons\\OneDrive - Universidad de La Frontera\\Universidad\\Sem8\\Análisis de Datos Práctico usando Python\\Tarea2\\Grupo_7_Tarea2.ipynb Celda 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sfons/OneDrive%20-%20Universidad%20de%20La%20Frontera/Universidad/Sem8/An%C3%A1lisis%20de%20Datos%20Pr%C3%A1ctico%20usando%20Python/Tarea2/Grupo_7_Tarea2.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m x_batch, y_batch \u001b[39min\u001b[39;00m train_dl:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sfons/OneDrive%20-%20Universidad%20de%20La%20Frontera/Universidad/Sem8/An%C3%A1lisis%20de%20Datos%20Pr%C3%A1ctico%20usando%20Python/Tarea2/Grupo_7_Tarea2.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     x_batch, y_batch \u001b[39m=\u001b[39m x_batch\u001b[39m.\u001b[39mto(device), y_batch\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sfons/OneDrive%20-%20Universidad%20de%20La%20Frontera/Universidad/Sem8/An%C3%A1lisis%20de%20Datos%20Pr%C3%A1ctico%20usando%20Python/Tarea2/Grupo_7_Tarea2.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(x_batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sfons/OneDrive%20-%20Universidad%20de%20La%20Frontera/Universidad/Sem8/An%C3%A1lisis%20de%20Datos%20Pr%C3%A1ctico%20usando%20Python/Tarea2/Grupo_7_Tarea2.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(pred, y_batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sfons/OneDrive%20-%20Universidad%20de%20La%20Frontera/Universidad/Sem8/An%C3%A1lisis%20de%20Datos%20Pr%C3%A1ctico%20usando%20Python/Tarea2/Grupo_7_Tarea2.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\sfons\\anaconda3\\envs\\ima539\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sfons\\anaconda3\\envs\\ima539\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sfons\\anaconda3\\envs\\ima539\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sfons\\anaconda3\\envs\\ima539\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\sfons\\anaconda3\\envs\\ima539\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 4\n",
    "hist = train(model, num_epochs, train_dl, valid_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ima539",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
